{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4b98b90",
   "metadata": {},
   "source": [
    "# RAPTOR Code Retrieval Demo\n",
    "\n",
    "This notebook demonstrates hierarchical code retrieval using RAPTOR clustering:\n",
    "1. Extract code from Python files\n",
    "2. Apply RAPTOR clustering for multi-level code understanding\n",
    "3. Store embeddings in FAISS vector store\n",
    "4. Retrieve relevant code snippets via semantic search\n",
    "\n",
    "**Key Features**:\n",
    "- Tree-structured retrieval at different abstraction levels\n",
    "- Semantic code search using natural language queries\n",
    "- Hierarchical summaries for codebase understanding\n",
    "- Local embeddings (no API costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d905c8",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "**Prerequisites:**\n",
    "1. Run `uv sync` in project root\n",
    "2. Copy `.env.example` to `.env` and configure API keys\n",
    "3. Specify Python codebase path for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24091df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parshav-potato/projects/western_digital_project/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path to import src modules\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# Remove cached modules to force fresh import\n",
    "modules_to_remove = [key for key in sys.modules.keys() if key.startswith('src.')]\n",
    "for module in modules_to_remove:\n",
    "    del sys.modules[module]\n",
    "\n",
    "# Import our modules\n",
    "from src.config import Config\n",
    "from src.raptor import RAPTORProcessor\n",
    "from src.vector_store import FAISSVectorStore\n",
    "from src.code_processor import CodeProcessor\n",
    "\n",
    "print(\"Modules loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538880b3",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Choose LLM provider for code summaries: `\"openai\"` or `\"gemini\"`\n",
    "\n",
    "Embeddings use local sentence-transformers (no API calls required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98a59a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded: Config(provider=gemini, model=gemini-2.0-flash)\n",
      "Embeddings: Local (sentence-transformers)\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "LLM_PROVIDER = \"gemini\"  # or \"openai\"\n",
    "USE_LOCAL_EMBEDDINGS = True  # Use free local embeddings\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config(llm_provider=LLM_PROVIDER, use_local_embeddings=USE_LOCAL_EMBEDDINGS)\n",
    "print(f\"Configuration loaded: {config}\")\n",
    "print(f\"Embeddings: {'Local (sentence-transformers)' if USE_LOCAL_EMBEDDINGS else f'{LLM_PROVIDER} API'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4892b0",
   "metadata": {},
   "source": [
    "## Set Codebase Path\n",
    "\n",
    "Specify the Python codebase directory for analysis (default: `../src`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c772665c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codebase path: ../\n",
      "Vector store directory: ../data/code_vector_store\n"
     ]
    }
   ],
   "source": [
    "# Set your codebase path\n",
    "CODEBASE_PATH = \"../\"  # Modify as needed\n",
    "\n",
    "# Output directories\n",
    "VECTOR_STORE_DIR = \"../data/code_vector_store\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(VECTOR_STORE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Codebase path: {CODEBASE_PATH}\")\n",
    "print(f\"Vector store directory: {VECTOR_STORE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3609d2a",
   "metadata": {},
   "source": [
    "## Step 1: Extract Code from Codebase\n",
    "\n",
    "Process:\n",
    "1. Find all Python files\n",
    "2. Extract functions and classes with docstrings\n",
    "3. Create structured code chunks for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62122b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 Python files\n",
      "\n",
      "Extracted 16 code chunks\n",
      "\n",
      "Sample code chunk:\n",
      "\n",
      "# File: example_usage.py\n",
      "\n",
      "Example script demonstrating PDF RAPTOR processing.\n",
      "Run this as: python example_usage.py\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Initialize code processor\n",
    "code_processor = CodeProcessor()\n",
    "\n",
    "# Extract code chunks\n",
    "code_chunks = code_processor.extract_code_chunks(CODEBASE_PATH)\n",
    "\n",
    "print(f\"\\nExtracted {len(code_chunks)} code chunks\")\n",
    "print(f\"\\nSample code chunk:\\n\")\n",
    "print(code_chunks[0][:500] + \"...\" if code_chunks else \"No chunks found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fd9cab",
   "metadata": {},
   "source": [
    "## Step 2: Apply RAPTOR Clustering\n",
    "\n",
    "RAPTOR creates hierarchical tree structure:\n",
    "- **Level 0 (Leaf)**: Individual functions/classes\n",
    "- **Level 1**: Summaries of related code groups\n",
    "- **Level 2**: High-level module summaries\n",
    "- **Level 3**: Overall codebase understanding\n",
    "\n",
    "Enables retrieval at different abstraction levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f410553e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Using local embeddings (sentence-transformers/all-MiniLM-L6-v2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parshav-potato/projects/western_digital_project/demo_code/src/config.py:58: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  return HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building RAPTOR tree structure...\n",
      "Creating hierarchical summaries of code.\n",
      "\n",
      "\n",
      "Building RAPTOR tree with 3 levels...\n",
      "Starting with 16 leaf texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parshav-potato/projects/western_digital_project/.venv/lib/python3.12/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Level 1: Generated 4 clusters\n",
      "  Level 2: Generated 1 clusters\n",
      "  Level 1: Added 4 summaries\n",
      "  Level 2: Added 1 summaries\n",
      "RAPTOR processing complete: 21 total texts\n",
      "\n",
      "RAPTOR Results:\n",
      "  Original code chunks: 16\n",
      "  Total texts (with summaries): 21\n",
      "  New summaries created: 5\n"
     ]
    }
   ],
   "source": [
    "# Initialize RAPTOR processor\n",
    "raptor = RAPTORProcessor(config)\n",
    "\n",
    "# Apply RAPTOR clustering (3 levels of hierarchy)\n",
    "print(\"Building RAPTOR tree structure...\")\n",
    "print(\"Creating hierarchical summaries of code.\\n\")\n",
    "\n",
    "all_code_texts = raptor.process(texts=code_chunks, n_levels=3)\n",
    "\n",
    "print(f\"\\nRAPTOR Results:\")\n",
    "print(f\"  Original code chunks: {len(code_chunks)}\")\n",
    "print(f\"  Total texts (with summaries): {len(all_code_texts)}\")\n",
    "print(f\"  New summaries created: {len(all_code_texts) - len(code_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97a5de8",
   "metadata": {},
   "source": [
    "## Step 3: Create FAISS Vector Store\n",
    "\n",
    "Store code chunks and summaries in FAISS vector database for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ae7d1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Using local embeddings (sentence-transformers/all-MiniLM-L6-v2)\n",
      "Creating vector store from code embeddings...\n",
      "\n",
      "Creating FAISS vector store from 21 texts...\n",
      "Vector store created successfully\n",
      "\n",
      "Vector Store Stats:\n",
      "  status: initialized\n",
      "  n_vectors: 21\n",
      "  embedding_provider: gemini\n"
     ]
    }
   ],
   "source": [
    "# Initialize vector store\n",
    "vector_store = FAISSVectorStore(config)\n",
    "\n",
    "# Create vector store from all texts\n",
    "print(\"Creating vector store from code embeddings...\")\n",
    "vector_store.create_from_texts(all_code_texts)\n",
    "\n",
    "# Display stats\n",
    "stats = vector_store.get_stats()\n",
    "print(f\"\\nVector Store Stats:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d27770",
   "metadata": {},
   "source": [
    "### Save Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46af435f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving vector store to ../data/code_vector_store...\n",
      "Vector store saved successfully\n",
      "Vector store saved to ../data/code_vector_store\n"
     ]
    }
   ],
   "source": [
    "# Save vector store for later use\n",
    "vector_store.save(VECTOR_STORE_DIR)\n",
    "print(f\"Vector store saved to {VECTOR_STORE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adc1f6b",
   "metadata": {},
   "source": [
    "## Step 4: Semantic Code Search\n",
    "\n",
    "Search for code using natural language queries.\n",
    "\n",
    "**Example queries:**\n",
    "- \"How to configure the LLM provider?\"\n",
    "- \"Code for processing PDF files\"\n",
    "- \"Functions that handle embeddings\"\n",
    "- \"RAPTOR clustering implementation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "450a1b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How to configure embeddings and LLM provider?\n",
      "\n",
      "\n",
      "Top 5 Results:\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Result 1 (Similarity Score: 1.0519):\n",
      "--------------------------------------------------------------------------------\n",
      "# File: src/config.py\n",
      "# Class: Config\n",
      "\n",
      "Configuration class for managing LLM providers and API keys.\n",
      "\n",
      "Methods: __init__, _validate_api_keys, get_embedding_model, get_llm_model, __repr__\n",
      "\n",
      "```python\n",
      "class Config:\n",
      "    \"\"\"Configuration class for managing LLM providers and API keys.\"\"\"\n",
      "    \n",
      "    def __init__(self, llm_provider: Optional[str] = None, use_local_embeddings: bool = False):\n",
      "        \"\"\"\n",
      "        Initialize configuration.\n",
      "        \n",
      "        Args:\n",
      "            llm_provider: Either \"openai\" or \"gemini\". If None, reads from env.\n",
      "            use_local_embeddings: If True, use local sentence-transformers instead of API embeddings\n",
      "        \"\"\"\n",
      "        self.llm_provider = llm_provider or os.getenv(\"LLM_PROVIDER\", \"openai\")\n",
      "        self.llm_provider = self.llm_provider.lower()\n",
      "        self.use_local_embeddings = use_local_embeddings\n",
      "        \n",
      "        if self.llm_provider not in [\"openai\", \"gemini\"]:\n",
      "            raise ValueError(f\"Invalid LLM provider: {self.llm_provider}. Must be 'openai' or 'gemini'\")\n",
      "        \n",
      "        # API Keys (only validate if not using local embeddings for everything)\n",
      "        self.openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
      "...\n",
      "```\n",
      "================================================================================\n",
      "\n",
      "Result 2 (Similarity Score: 1.0794):\n",
      "--------------------------------------------------------------------------------\n",
      "The provided content describes a configuration module (`src/config.py`) and a vector store module (`src/vector_store.py`).\n",
      "\n",
      "**`src/config.py`:**\n",
      "\n",
      "*   Defines a `Config` class for managing LLM providers (either \"openai\" or \"gemini\") and API keys.\n",
      "*   The `Config` class has an `__init__` method that takes `llm_provider` (defaults to \"openai\" or reads from the environment variable `LLM_PROVIDER`) and `use_local_embeddings` as arguments. It initializes the configuration, validates the `llm_provider`, and retrieves API keys from environment variables.\n",
      "*   The `Config` class also defines a `GeminiWrapper` class, which inherits from `ChatGoogleGenerativeAI`. The `GeminiWrapper` class overrides the `_generate` method to remove `max_retries` from the keyword arguments before passing them to the parent class.\n",
      "*   The `Config` class has a `__repr__` method that returns a string representation of the `Config` object, including the provider and model name.\n",
      "\n",
      "**`src/vector_store.py`:**\n",
      "\n",
      "*   Defines a `FAISSVectorStore` class for managing a FAISS vector store for document embeddings.\n",
      "*   The `FAISSVectorStore` class has an `__init__` method that takes a `Config` instance as an argument. It initializes the vector store with the embedding model obtained from the `Config` instance.\n",
      "*   The `FAISSVectorStore` class has a `create_from_texts` method that takes a list of texts and optional metadata and creates a FAISS index from them.\n",
      "================================================================================\n",
      "\n",
      "Result 3 (Similarity Score: 1.1159):\n",
      "--------------------------------------------------------------------------------\n",
      "# File: src/config.py\n",
      "\n",
      "Configuration module for managing API keys and LLM provider selection.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Result 4 (Similarity Score: 1.3469):\n",
      "--------------------------------------------------------------------------------\n",
      "# File: src/vector_store.py\n",
      "# Class: FAISSVectorStore\n",
      "\n",
      "Manage FAISS vector store for document embeddings.\n",
      "\n",
      "Methods: __init__, create_from_texts, create_from_documents, add_texts, add_documents, similarity_search, similarity_search_with_score, save, load, get_stats\n",
      "\n",
      "```python\n",
      "class FAISSVectorStore:\n",
      "    \"\"\"Manage FAISS vector store for document embeddings.\"\"\"\n",
      "    \n",
      "    def __init__(self, config):\n",
      "        \"\"\"\n",
      "        Initialize FAISS vector store.\n",
      "        \n",
      "        Args:\n",
      "            config: Config instance with embedding settings\n",
      "        \"\"\"\n",
      "        self.config = config\n",
      "        self.embeddings = config.get_embedding_model()\n",
      "        self.vector_store = None\n",
      "        \n",
      "    def create_from_texts(\n",
      "        self,\n",
      "        texts: List[str],\n",
      "        metadatas: Optional[List[dict]] = None\n",
      "    ) -> FAISS:\n",
      "        \"\"\"\n",
      "...\n",
      "```\n",
      "================================================================================\n",
      "\n",
      "Result 5 (Similarity Score: 1.5311):\n",
      "--------------------------------------------------------------------------------\n",
      "The provided content describes a Python script (`example_usage.py`) demonstrating the use of a PDF processing tool called \"PDF RAPTOR\". The script initializes by printing a header. It then defines configuration variables, including the LLM provider (defaulting to \"openai\", but can be changed to \"gemini\"), the PDF file path (`data/your_document.pdf`), the image directory (`data/images`), and the vector store directory (`data/vector_store`). The script also includes a `main` function, though its full implementation is not shown.\n",
      "\n",
      "The content also describes a `PDFProcessor` class (located in `src/pdf_processor.py`) designed to extract text, tables, and images from PDF files. The class has an `__init__` method that initializes the processor with a configuration object (`config`) containing LLM settings. It also initializes an LLM model (with temperature 0 and max_tokens 1024), a `batch_size` of 10, and a `delay_between_batches` of 1 second. The `extract_elements` method is responsible for extracting raw elements from the PDF.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query = \"How to configure embeddings and LLM provider?\"  # Modify as needed\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "# Search for similar code\n",
    "results = vector_store.similarity_search_with_score(query, k=5)\n",
    "\n",
    "print(f\"\\nTop {len(results)} Results:\\n\")\n",
    "print(\"=\" * 80)\n",
    "for i, (doc, score) in enumerate(results, 1):\n",
    "    print(f\"\\nResult {i} (Similarity Score: {score:.4f}):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(doc.page_content)\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36953a57",
   "metadata": {},
   "source": [
    "### Additional Example Queries\n",
    "\n",
    "Try different types of queries to see the power of RAPTOR retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5fdf015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Query: PDF processing and extraction\n",
      "================================================================================\n",
      "\n",
      "Result 1 (Score: 0.7873):\n",
      "# File: src/pdf_processor.py  PDF processing module for extracting text, tables, and images. ...\n",
      "\n",
      "Result 2 (Score: 0.8754):\n",
      "The provided content describes a Python package named `src` containing modules for processing PDFs, code, documentation, and managing vector stores, all designed to work with a system called PDF RAPTO...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Query: RAPTOR clustering algorithm\n",
      "================================================================================\n",
      "\n",
      "Result 1 (Score: 0.8256):\n",
      "# File: src/raptor.py # Class: RAPTORProcessor  RAPTOR hierarchical text clustering and summarization.  Methods: __init__, embed_texts, global_cluster_embeddings, local_cluster_embeddings, get_optimal...\n",
      "\n",
      "Result 2 (Score: 0.9356):\n",
      "# File: src/raptor.py  RAPTOR (Recursive Abstractive Processing for Tree-Organized Retrieval) implementation. Performs hierarchical clustering and summarization of text documents. ...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Query: Vector store implementation\n",
      "================================================================================\n",
      "\n",
      "Result 1 (Score: 1.0676):\n",
      "# File: src/vector_store.py  FAISS vector store module for efficient similarity search. ...\n",
      "\n",
      "Result 2 (Score: 1.2058):\n",
      "The provided content describes a configuration module (`src/config.py`) and a vector store module (`src/vector_store.py`).  **`src/config.py`:**  *   Defines a `Config` class for managing LLM provider...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Query: How to handle batch processing?\n",
      "================================================================================\n",
      "\n",
      "Result 1 (Score: 1.2123):\n",
      "# File: src/pdf_processor.py # Class: PDFProcessor  Process PDF files to extract text, tables, and images.  Methods: __init__, extract_elements, _summarize_element, _summarize_image, _encode_image, _h...\n",
      "\n",
      "Result 2 (Score: 1.3460):\n",
      "The provided content describes a Python package named `src` containing modules for processing PDFs, code, documentation, and managing vector stores, all designed to work with a system called PDF RAPTO...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try multiple queries\n",
    "example_queries = [\n",
    "    \"PDF processing and extraction\",\n",
    "    \"RAPTOR clustering algorithm\",\n",
    "    \"Vector store implementation\",\n",
    "    \"How to handle batch processing?\",\n",
    "]\n",
    "\n",
    "for query in example_queries:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    results = vector_store.similarity_search_with_score(query, k=2)\n",
    "    \n",
    "    for i, (doc, score) in enumerate(results, 1):\n",
    "        print(f\"Result {i} (Score: {score:.4f}):\")\n",
    "        # Show first 200 chars of the result\n",
    "        preview = doc.page_content[:200].replace('\\n', ' ')\n",
    "        print(f\"{preview}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74745725",
   "metadata": {},
   "source": [
    "## Advanced: Tree-Level Retrieval\n",
    "\n",
    "RAPTOR enables retrieval at different abstraction levels:\n",
    "- Higher-level results provide summaries\n",
    "- Lower-level results provide specific code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77e2e38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How does the configuration system work?\n",
      "\n",
      "\n",
      "Result 1 (Similarity: -0.115):\n",
      "--------------------------------------------------------------------------------\n",
      "Level: Mid-level Summary\n",
      "\n",
      "Content:\n",
      "The provided content describes a configuration module (`src/config.py`) and a vector store module (`src/vector_store.py`).\n",
      "\n",
      "**`src/config.py`:**\n",
      "\n",
      "*   Defines a `Config` class for managing LLM providers (either \"openai\" or \"gemini\") and API keys.\n",
      "*   The `Config` class has an `__init__` method that takes `llm_provider` (defaults to \"openai\" or reads from the environment variable `LLM_PROVIDER`) and...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result 2 (Similarity: -0.350):\n",
      "--------------------------------------------------------------------------------\n",
      "Level: Mid-level Summary\n",
      "\n",
      "Content:\n",
      "The provided content describes a Python script (`example_usage.py`) demonstrating the use of a PDF processing tool called \"PDF RAPTOR\". The script initializes by printing a header. It then defines configuration variables, including the LLM provider (defaulting to \"openai\", but can be changed to \"gemini\"), the PDF file path (`data/your_document.pdf`), the image directory (`data/images`), and the ve...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result 3 (Similarity: -0.412):\n",
      "--------------------------------------------------------------------------------\n",
      "Level: Leaf (Original Code)\n",
      "\n",
      "Content:\n",
      "# File: src/config.py\n",
      "\n",
      "Configuration module for managing API keys and LLM provider selection.\n",
      "...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def search_with_context(query: str, k: int = 3):\n",
    "    \"\"\"\n",
    "    Search and display results with abstraction level metadata.\n",
    "    \"\"\"\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    \n",
    "    results = vector_store.similarity_search_with_score(query, k=k)\n",
    "    \n",
    "    for i, (doc, score) in enumerate(results, 1):\n",
    "        print(f\"\\nResult {i} (Similarity: {1-score:.3f}):\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Determine if it's original code or a summary\n",
    "        content = doc.page_content\n",
    "        if \"# File:\" in content or \"```python\" in content:\n",
    "            level = \"Leaf (Original Code)\"\n",
    "        elif len(content) < 300:\n",
    "            level = \"High-level Summary\"\n",
    "        else:\n",
    "            level = \"Mid-level Summary\"\n",
    "        \n",
    "        print(f\"Level: {level}\")\n",
    "        print(f\"\\nContent:\\n{content[:400]}...\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Try it out\n",
    "search_with_context(\"How does the configuration system work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e5f19b",
   "metadata": {},
   "source": [
    "## Interactive Code Search\n",
    "\n",
    "Run this cell multiple times with different queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c78974a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Show me error handling code\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Result 1:\n",
      "--------------------------------------------------------------------------------\n",
      "# File: src/code_processor.py\n",
      "\n",
      "Code processing module for extracting code from Python files.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Result 2:\n",
      "--------------------------------------------------------------------------------\n",
      "# File: src/code_processor.py\n",
      "# Class: CodeProcessor\n",
      "\n",
      "Extract and process code from Python codebases.\n",
      "\n",
      "Methods: __init__, extract_code_chunks, _extract_from_file, _extract_class, _extract_function, get_file_stats\n",
      "\n",
      "```python\n",
      "class CodeProcessor:\n",
      "    \"\"\"Extract and process code from Python codebases.\"\"\"\n",
      "    \n",
      "    def __init__(self):\n",
      "        \"\"\"Initialize code processor.\"\"\"\n",
      "        pass\n",
      "    \n",
      "    def extract_code_chunks(\n",
      "        self, \n",
      "        codebase_path: str, \n",
      "        max_files: Optional[int] = None\n",
      "    ) -> List[str]:\n",
      "        \"\"\"\n",
      "        Extract code chunks from Python files in a codebase.\n",
      "        \n",
      "        Extracts:\n",
      "        - Module-level docstrings\n",
      "        - Classes with their docstrings, methods, and source code\n",
      "        - Top-level functions with docstrings, arguments, and source code\n",
      "        \n",
      "...\n",
      "```\n",
      "================================================================================\n",
      "\n",
      "Result 3:\n",
      "--------------------------------------------------------------------------------\n",
      "# File: example_usage.py\n",
      "\n",
      "Example script demonstrating PDF RAPTOR processing.\n",
      "Run this as: python example_usage.py\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Interactive search\n",
    "your_query = \"Show me error handling code\"  # Modify as needed\n",
    "\n",
    "results = vector_store.similarity_search(your_query, k=3)\n",
    "\n",
    "print(f\"Query: {your_query}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(doc.page_content)\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e906bc3",
   "metadata": {},
   "source": [
    "## Load Existing Vector Store (Optional)\n",
    "\n",
    "Load previously saved vector store to skip processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13e39422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to load existing vector store\n",
    "# vector_store_loaded = FAISSVectorStore(config)\n",
    "# vector_store_loaded.load(VECTOR_STORE_DIR)\n",
    "\n",
    "# print(\"Vector store loaded successfully\")\n",
    "# print(f\"Stats: {vector_store_loaded.get_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590d0085",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Successfully completed:\n",
    "- Extracted code from Python codebase\n",
    "- Applied RAPTOR hierarchical clustering\n",
    "- Created FAISS vector store for code search\n",
    "- Performed semantic code retrieval\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Try different codebases** - Point to any Python project\n",
    "2. **Integrate with RAG** - Build code Q&A systems\n",
    "3. **Code documentation** - Generate docs from RAPTOR summaries\n",
    "4. **Code search tools** - Build IDE plugins\n",
    "5. **Multi-language support** - Extend to JavaScript, Java, etc.\n",
    "\n",
    "### Optimization Tips:\n",
    "\n",
    "- **Larger codebases**: Use `max_files` parameter or filters\n",
    "- **Better results**: Include imports and comments in extraction\n",
    "- **Custom embeddings**: Try code-specific embedding models\n",
    "- **Fine-tune retrieval**: Adjust `k` parameter and similarity thresholds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
