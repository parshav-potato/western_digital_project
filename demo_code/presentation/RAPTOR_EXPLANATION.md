# RAPTOR: Complete End-to-End Explanation

## ğŸ“š Table of Contents
1. [The Problem](#the-problem)
2. [The Solution](#the-solution)
3. [How RAPTOR Works](#how-raptor-works)
4. [Implementation Details](#implementation-details)
5. [Code Flow](#code-flow)
6. [Presentation Guide](#presentation-guide)

---

## ğŸ¯ The Problem

### Traditional RAG Pipeline Issues:

```
Document â†’ Split into chunks â†’ Embed â†’ Store in Vector DB
                â†“
            Search by similarity
                â†“
        Retrieve individual chunks
```

**Problems:**
1. **Loss of Context**: Individual chunks lack broader document context
2. **Scattered Information**: Related concepts spread across multiple chunks
3. **Single Granularity**: Can only retrieve at one level of detail
4. **High-level Questions Fail**: "What is this document about?" hard to answer

### Example:
```
Document: "Attention Is All You Need" Paper

Chunk 1: "The Transformer uses multi-head attention..."
Chunk 2: "Positional encoding is added to input embeddings..."
Chunk 3: "The encoder consists of 6 identical layers..."

âŒ Query: "What is the main contribution?"
   â†’ Returns: Random individual chunks (lacks overview)
```

---

## âœ… The Solution: RAPTOR

**RAPTOR** = **R**ecursive **A**bstractive **P**rocessing for **T**ree-**O**rganized **R**etrieval

### Key Innovation:
Build a **hierarchical tree** where each level represents a different abstraction level.

```
Level 2 (High-level):  "Paper introduces Transformer, attention-based
                        architecture replacing RNNs for seq2seq tasks"
                                    â†‘
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚           â”‚           â”‚
Level 1 (Mid-level):   Summary 1  Summary 2  Summary 3
                        â†‘           â†‘           â†‘
                    â”Œâ”€â”€â”€â”¼â”€â”€â”€â”   â”Œâ”€â”€â”€â”¼â”€â”€â”€â”   â”Œâ”€â”€â”€â”¼â”€â”€â”€â”
Level 0 (Details):  C1  C2  C3  C4  C5  C6  C7  C8  C9

All levels stored in vector database!
```

### Benefits:
âœ… **Multi-level Retrieval**: Can match queries at appropriate abstraction level
âœ… **Preserved Context**: Related chunks clustered and summarized together
âœ… **Better High-level Answers**: Summaries provide document overview
âœ… **Flexible Granularity**: Retrieve details OR big picture

---

## ğŸ”¬ How RAPTOR Works

### Step-by-Step Process:

#### **Step 1: Start with Text Chunks (Level 0)**
```python
texts = ["chunk1", "chunk2", "chunk3", ..., "chunk30"]
# These are your original document chunks (leaf nodes)
```

#### **Step 2: Embed All Chunks**
```python
embeddings = embedding_model.embed(texts)
# Convert text to vectors (384 dimensions with sentence-transformers)
```

#### **Step 3: Dimensionality Reduction (UMAP)**
```python
# High-dimensional embeddings â†’ Lower dimensions for clustering
reduced_embeddings = UMAP(n_components=10).fit_transform(embeddings)
```

**Why?** Clustering works better in lower dimensions (curse of dimensionality)

#### **Step 4: Clustering (Gaussian Mixture Model)**
```python
# Find optimal number of clusters using BIC
optimal_k = find_optimal_clusters(reduced_embeddings)

# Cluster similar chunks together
gmm = GaussianMixture(n_components=optimal_k)
cluster_labels = gmm.fit_predict(reduced_embeddings)

# Example: 30 chunks â†’ 5 clusters
Cluster 0: [chunk1, chunk5, chunk7]    # About attention mechanism
Cluster 1: [chunk2, chunk4, chunk9]    # About model architecture
Cluster 2: [chunk3, chunk6, chunk8]    # About training details
...
```

#### **Step 5: Hierarchical Clustering**

**Global + Local Strategy:**
```python
# First: Global clustering (high-level groups)
global_clusters = cluster(embeddings)

# Then: Local clustering within each global cluster (fine-grained)
for global_cluster in global_clusters:
    local_clusters = cluster(global_cluster_embeddings)
```

This creates more nuanced groupings!

#### **Step 6: Summarize Each Cluster**
```python
for cluster in clusters:
    # Combine all texts in cluster
    cluster_text = " --- --- ".join(cluster_texts)

    # Generate summary using LLM
    summary = llm.generate(
        "Please provide a comprehensive summary: " + cluster_text
    )

    summaries.append(summary)

# Example:
# Cluster 0 texts â†’ "This section describes the multi-head attention
#                    mechanism which allows the model to jointly attend
#                    to information from different representation
#                    subspaces..."
```

#### **Step 7: Recurse! (Build Higher Levels)**
```python
# Level 1 summaries become inputs for Level 2
level_1_summaries = [summary1, summary2, summary3, summary4, summary5]

# Repeat clustering and summarization
level_2_summaries = cluster_and_summarize(level_1_summaries)

# Continue until you have 1 cluster or reach max levels
```

#### **Step 8: Store Everything in Vector DB**
```python
all_texts = (
    original_chunks +      # Level 0: 30 texts
    level_1_summaries +    # Level 1: 5 texts
    level_2_summaries      # Level 2: 1 text
)
# Total: 36 texts in vector store!

vector_store = FAISS.from_texts(all_texts, embeddings)
```

---

## ğŸ”§ Implementation Details

### Core Components:

#### 1. **RAPTOR Processor** (`src/raptor.py`)

**Key Methods:**

```python
class RAPTORProcessor:
    def embed_texts(texts):
        """Convert texts to vector embeddings"""

    def global_cluster_embeddings(embeddings, dim):
        """UMAP dimensionality reduction (global view)"""

    def local_cluster_embeddings(embeddings, dim):
        """UMAP dimensionality reduction (local view)"""

    def get_optimal_clusters(embeddings):
        """Find optimal K using BIC (Bayesian Information Criterion)"""

    def gmm_cluster(embeddings, threshold):
        """Cluster embeddings using Gaussian Mixture Model"""

    def perform_clustering(embeddings):
        """Global + Local hierarchical clustering"""

    def embed_cluster_texts(texts):
        """Embed texts and assign cluster labels"""

    def embed_cluster_summarize_texts(texts, level):
        """Main processing: embed â†’ cluster â†’ summarize"""

    def recursive_embed_cluster_summarize(texts, level, n_levels):
        """Recursively build tree structure"""

    def process(texts, n_levels=3):
        """Entry point: process texts through RAPTOR"""
```

#### 2. **PDF Processor** (`src/pdf_processor.py`)
```python
class PDFProcessor:
    def extract_elements(pdf_path):
        """Extract text, tables, images from PDF"""

    def process_pdf(pdf_path):
        """Main entry: PDF â†’ text chunks"""
```

#### 3. **Vector Store** (`src/vector_store.py`)
```python
class FAISSVectorStore:
    def create_from_texts(texts):
        """Build FAISS index from texts"""

    def similarity_search(query, k=5):
        """Find k most similar texts to query"""

    def save(path):
        """Persist vector store to disk"""
```

#### 4. **Configuration** (`src/config.py`)
```python
class Config:
    def get_llm_model():
        """Get OpenAI or Gemini LLM"""

    def get_embedding_model():
        """Get embedding model (API or local)"""
```

---

## ğŸ¬ Complete Code Flow

### Full Pipeline Execution:

```python
# 1. CONFIGURATION
config = Config(
    llm_provider="gemini",
    use_local_embeddings=True  # Free!
)

# 2. PDF EXTRACTION
pdf_processor = PDFProcessor(config)
documents, raw_texts = pdf_processor.process_pdf("paper.pdf")
# â†’ Returns: ["chunk1", "chunk2", ..., "chunk30"]

# 3. RAPTOR PROCESSING
raptor = RAPTORProcessor(config)
all_texts = raptor.process(raw_texts, n_levels=3)

# Behind the scenes:
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ Level 0: 30 original chunks         â”‚
# â”‚   â†“ embed â†’ cluster â†’ summarize     â”‚
# â”‚ Level 1: 5 summaries                â”‚
# â”‚   â†“ embed â†’ cluster â†’ summarize     â”‚
# â”‚ Level 2: 1 high-level summary       â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
# Returns: [chunk1, ..., chunk30, summary1, ..., summary5, summary6]

# 4. VECTOR STORE CREATION
vector_store = FAISSVectorStore(config)
vector_store.create_from_texts(all_texts)
vector_store.save("data/vector_store")

# 5. QUERYING
results = vector_store.similarity_search(
    "What is the main contribution?",
    k=5
)
# Returns: [most_relevant_text1, most_relevant_text2, ...]
```

### What Happens During `raptor.process()`:

```python
def process(texts, n_levels=3):
    # Level 1
    embeddings = embed(texts)                    # 30 texts â†’ vectors
    clusters = cluster(embeddings)               # 30 texts â†’ 5 clusters
    summaries_L1 = summarize(clusters)           # 5 summaries

    # Level 2
    embeddings = embed(summaries_L1)             # 5 summaries â†’ vectors
    clusters = cluster(embeddings)               # 5 summaries â†’ 1 cluster
    summaries_L2 = summarize(clusters)           # 1 summary

    # Combine all
    return texts + summaries_L1 + summaries_L2   # 30 + 5 + 1 = 36 texts
```

---

## ğŸ¤ Presentation Guide

### **Slide 1: Title**
```
RAPTOR: Tree-Organized Retrieval for RAG
Hierarchical Document Understanding at Scale
```

### **Slide 2: The Problem with Traditional RAG**
```
âŒ Traditional RAG Limitations:
   â€¢ Individual chunks lack context
   â€¢ Related information scattered
   â€¢ Single granularity (details only)
   â€¢ High-level questions fail

Example: "What is this paper about?"
â†’ Returns random detail chunks, not overview
```

### **Slide 3: RAPTOR's Innovation**
```
âœ… RAPTOR Solution:
   â€¢ Build hierarchical tree structure
   â€¢ Multiple abstraction levels
   â€¢ Cluster related information
   â€¢ Generate summaries at each level

[Show tree diagram: Level 0 â†’ Level 1 â†’ Level 2]
```

### **Slide 4: How It Works (Algorithm)**
```
1. Embed text chunks (Level 0)
2. Cluster similar chunks together
3. Summarize each cluster â†’ Level 1
4. Repeat: Cluster summaries â†’ Level 2
5. Store ALL levels in vector database

Result: 30 chunks + 5 summaries + 1 overview = 36 searchable texts
```

### **Slide 5: Technical Components**
```
â€¢ UMAP: Dimensionality reduction (384D â†’ 10D)
â€¢ GMM: Gaussian Mixture Model clustering
â€¢ BIC: Find optimal number of clusters
â€¢ LLM: Generate cluster summaries (Gemini/GPT)
â€¢ FAISS: Fast similarity search
```

### **Slide 6: Demo Results**
```
Input: "Attention Is All You Need" paper (15 pages)

Output:
â€¢ Level 0: 30 text chunks (details)
â€¢ Level 1: 5 cluster summaries (sections)
â€¢ Level 2: 1 high-level summary (overview)

Query: "What is the main contribution?"
â†’ Returns Level 2 summary: "Paper introduces
   Transformer architecture..."
```

### **Slide 7: Benefits**
```
âœ… Better context preservation
âœ… Multi-level retrieval
âœ… Improved high-level Q&A
âœ… Efficient: Local embeddings (FREE)
âœ… No database: FAISS (local files)
```

### **Slide 8: Use Cases**
```
â€¢ Research paper analysis
â€¢ Technical documentation Q&A
â€¢ Legal document review
â€¢ Code repository understanding
â€¢ Knowledge base construction
```

### **Slide 9: Implementation**
```
Tech Stack:
â€¢ Python 3.12
â€¢ LangChain (LLM orchestration)
â€¢ FAISS (vector store)
â€¢ sentence-transformers (embeddings)
â€¢ UMAP + scikit-learn (clustering)
â€¢ Google Gemini / OpenAI (LLM)
```

### **Slide 10: Results & Metrics**
```
Performance:
â€¢ Processing: 30 chunks â†’ 36 texts in ~2 minutes
â€¢ Storage: 36 vectors (384 dimensions each)
â€¢ Search: <10ms per query
â€¢ Cost: FREE (with local embeddings)
```

---

## ğŸ¯ Key Talking Points

### **Why RAPTOR Matters:**

1. **Context is King**: Traditional chunking loses document structure. RAPTOR preserves it through hierarchical summaries.

2. **Flexible Granularity**: Different queries need different detail levels:
   - "What is this about?" â†’ High-level summary
   - "How does attention work?" â†’ Detailed chunks

3. **Intelligent Grouping**: Clustering ensures related concepts are summarized together, not scattered.

4. **Scalable**: Works on small documents (15 pages) and large codebases (1000s of files).

### **Technical Innovations:**

1. **Global + Local Clustering**: Two-stage process captures both broad themes and fine details.

2. **Optimal Cluster Selection**: BIC automatically determines best number of clusters (no manual tuning).

3. **Recursive Architecture**: Tree structure naturally represents document hierarchy.

4. **Cost-Effective**: Local embeddings eliminate per-query API costs.

---

## ğŸ’¡ Demo Talking Points

### **Live Demo Script:**

```bash
# 1. Show the input
"We have the 'Attention Is All You Need' paper - 15 pages, highly technical"

# 2. Show processing
"Watch RAPTOR build the tree structure:
 - Level 0: 30 original chunks
 - Level 1: 5 cluster summaries
 - Level 2: 1 high-level summary
 Total: 36 texts in vector store"

# 3. Show query results
"Query: 'What is the main contribution?'
 â†’ Returns Level 2 summary (high-level overview)

 Query: 'How does multi-head attention work?'
 â†’ Returns Level 0 chunks (technical details)"

# 4. Show advantages
"This would be impossible with traditional chunking alone!"
```

---

## ğŸ“Š Visual Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAPTOR PIPELINE                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INPUT: PDF Document
    â†“
[PDF Processor]
    â†“
30 Text Chunks (Level 0)
    â†“
[Embedding] â†’ 30 vectors (384D)
    â†“
[UMAP] â†’ 30 vectors (10D)
    â†“
[GMM Clustering] â†’ 5 clusters
    â†“
[LLM Summarization] â†’ 5 summaries (Level 1)
    â†“
[Embedding] â†’ 5 vectors (384D)
    â†“
[UMAP] â†’ 5 vectors (10D)
    â†“
[GMM Clustering] â†’ 1 cluster
    â†“
[LLM Summarization] â†’ 1 summary (Level 2)
    â†“
[Combine All Levels]
    â†“
36 texts (30 + 5 + 1)
    â†“
[FAISS Vector Store]
    â†“
[Similarity Search] â† User Query
    â†“
Retrieved Results (Multi-level)
```

---

## ğŸš€ Quick Start for Demo

```bash
# 1. Setup
cd /home/aditya/Documents/western_digital_project/demo_code
source .venv/bin/activate

# 2. Run demo
python run_demo_simple.py

# 3. Query the vector store
python -c "
from src.config import Config
from src.vector_store import FAISSVectorStore

config = Config(llm_provider='gemini', use_local_embeddings=True)
store = FAISSVectorStore(config)
store.load('data/vector_store_simple')

# Try different queries
queries = [
    'What is the main contribution?',
    'How does attention mechanism work?',
    'What are the model components?'
]

for q in queries:
    print(f'\nQuery: {q}')
    results = store.similarity_search(q, k=2)
    for i, doc in enumerate(results, 1):
        print(f'Result {i}: {doc.page_content[:200]}...')
"
```

---

## ğŸ“ Summary

**RAPTOR transforms RAG from flat chunks to intelligent hierarchies:**

```
Traditional:  [C1] [C2] [C3] [C4] [C5] ... [C30]
                â†“
              Single-level retrieval

RAPTOR:      [High-level overview]
                     â†“
            [Section summaries]
                     â†“
           [Detailed chunks]
                     â†“
        Multi-level intelligent retrieval
```

**Result:** Better context, better answers, better RAG! ğŸ¯
