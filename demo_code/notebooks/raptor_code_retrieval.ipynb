{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4b98b90",
   "metadata": {},
   "source": [
    "# RAPTOR Code Retrieval Demo\n",
    "\n",
    "This notebook demonstrates hierarchical code retrieval using RAPTOR clustering:\n",
    "1. Extract code from Python files\n",
    "2. Apply RAPTOR clustering for multi-level code understanding\n",
    "3. Store embeddings in FAISS vector store\n",
    "4. Retrieve relevant code snippets via semantic search\n",
    "\n",
    "**Key Features**:\n",
    "- Tree-structured retrieval at different abstraction levels\n",
    "- Semantic code search using natural language queries\n",
    "- Hierarchical summaries for codebase understanding\n",
    "- Local embeddings (no API costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d905c8",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "**Prerequisites:**\n",
    "1. Run `uv sync` in project root\n",
    "2. Copy `.env.example` to `.env` and configure API keys\n",
    "3. Specify Python codebase path for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24091df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parshav-potato/projects/wd_research/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modules loaded fresh (cache cleared)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path to import src modules\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# Remove cached modules to force fresh import\n",
    "modules_to_remove = [key for key in sys.modules.keys() if key.startswith('src.')]\n",
    "for module in modules_to_remove:\n",
    "    del sys.modules[module]\n",
    "\n",
    "# Import our modules\n",
    "from src.config import Config\n",
    "from src.raptor import RAPTORProcessor\n",
    "from src.vector_store import FAISSVectorStore\n",
    "from src.code_processor import CodeProcessor\n",
    "\n",
    "print(\"Modules loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538880b3",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Choose LLM provider for code summaries: `\"openai\"` or `\"gemini\"`\n",
    "\n",
    "Embeddings use local sentence-transformers (no API calls required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a59a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded: Config(provider=gemini, model=gemini-2.0-flash)\n",
      "   üìä Embeddings: Local (sentence-transformers)\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "LLM_PROVIDER = \"gemini\"  # or \"openai\"\n",
    "USE_LOCAL_EMBEDDINGS = True  # Use free local embeddings\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config(llm_provider=LLM_PROVIDER, use_local_embeddings=USE_LOCAL_EMBEDDINGS)\n",
    "print(f\"Configuration loaded: {config}\")\n",
    "print(f\"Embeddings: {'Local (sentence-transformers)' if USE_LOCAL_EMBEDDINGS else f'{LLM_PROVIDER} API'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4892b0",
   "metadata": {},
   "source": [
    "## Set Codebase Path\n",
    "\n",
    "Specify the Python codebase directory for analysis (default: `../src`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c772665c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Codebase path: ../src\n",
      "üíæ Vector store will be saved to: ../data/code_vector_store\n"
     ]
    }
   ],
   "source": [
    "# Set your codebase path\n",
    "CODEBASE_PATH = \"../src\"  # Modify as needed\n",
    "\n",
    "# Output directories\n",
    "VECTOR_STORE_DIR = \"../data/code_vector_store\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(VECTOR_STORE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Codebase path: {CODEBASE_PATH}\")\n",
    "print(f\"Vector store directory: {VECTOR_STORE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3609d2a",
   "metadata": {},
   "source": [
    "## Step 1: Extract Code from Codebase\n",
    "\n",
    "Process:\n",
    "1. Find all Python files\n",
    "2. Extract functions and classes with docstrings\n",
    "3. Create structured code chunks for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62122b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Found 5 Python files\n",
      "\n",
      "üìä Extracted 10 code chunks\n",
      "\n",
      "üìù Sample code chunk:\n",
      "\n",
      "# File: config.py\n",
      "\n",
      "Configuration module for managing API keys and LLM provider selection.\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Initialize code processor\n",
    "code_processor = CodeProcessor()\n",
    "\n",
    "# Extract code chunks\n",
    "code_chunks = code_processor.extract_code_chunks(CODEBASE_PATH)\n",
    "\n",
    "print(f\"\\nExtracted {len(code_chunks)} code chunks\")\n",
    "print(f\"\\nSample code chunk:\\n\")\n",
    "print(code_chunks[0][:500] + \"...\" if code_chunks else \"No chunks found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fd9cab",
   "metadata": {},
   "source": [
    "## Step 2: Apply RAPTOR Clustering\n",
    "\n",
    "RAPTOR creates hierarchical tree structure:\n",
    "- **Level 0 (Leaf)**: Individual functions/classes\n",
    "- **Level 1**: Summaries of related code groups\n",
    "- **Level 2**: High-level module summaries\n",
    "- **Level 3**: Overall codebase understanding\n",
    "\n",
    "Enables retrieval at different abstraction levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f410553e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Using local embeddings (sentence-transformers/all-MiniLM-L6-v2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parshav-potato/projects/wd_research/demo_code/src/config.py:58: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  return HuggingFaceEmbeddings(\n",
      "/home/parshav-potato/projects/wd_research/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/parshav-potato/projects/wd_research/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå≥ Building RAPTOR tree structure...\n",
      "This will create hierarchical summaries of your code.\n",
      "\n",
      "\n",
      "üå≥ Building RAPTOR tree with 3 levels...\n",
      "üìä Starting with 10 leaf texts\n",
      "  Level 1: Generated 1 clusters\n",
      "  Level 1: Added 1 summaries\n",
      "‚úÖ RAPTOR processing complete: 11 total texts\n",
      "\n",
      "üìä RAPTOR Results:\n",
      "  Original code chunks: 10\n",
      "  Total texts (with summaries): 11\n",
      "  New summaries created: 1\n"
     ]
    }
   ],
   "source": [
    "# Initialize RAPTOR processor\n",
    "raptor = RAPTORProcessor(config)\n",
    "\n",
    "# Apply RAPTOR clustering (3 levels of hierarchy)\n",
    "print(\"Building RAPTOR tree structure...\")\n",
    "print(\"Creating hierarchical summaries of code.\\n\")\n",
    "\n",
    "all_code_texts = raptor.process(texts=code_chunks, n_levels=3)\n",
    "\n",
    "print(f\"\\nRAPTOR Results:\")\n",
    "print(f\"  Original code chunks: {len(code_chunks)}\")\n",
    "print(f\"  Total texts (with summaries): {len(all_code_texts)}\")\n",
    "print(f\"  New summaries created: {len(all_code_texts) - len(code_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97a5de8",
   "metadata": {},
   "source": [
    "## Step 3: Create FAISS Vector Store\n",
    "\n",
    "Store code chunks and summaries in FAISS vector database for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae7d1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Using local embeddings (sentence-transformers/all-MiniLM-L6-v2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parshav-potato/projects/wd_research/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Creating vector store from code embeddings...\n",
      "\n",
      "üîç Creating FAISS vector store from 11 texts...\n",
      "‚úÖ Vector store created successfully\n",
      "\n",
      "üìä Vector Store Stats:\n",
      "  status: initialized\n",
      "  n_vectors: 11\n",
      "  embedding_provider: gemini\n"
     ]
    }
   ],
   "source": [
    "# Initialize vector store\n",
    "vector_store = FAISSVectorStore(config)\n",
    "\n",
    "# Create vector store from all texts\n",
    "print(\"Creating vector store from code embeddings...\")\n",
    "vector_store.create_from_texts(all_code_texts)\n",
    "\n",
    "# Display stats\n",
    "stats = vector_store.get_stats()\n",
    "print(f\"\\nVector Store Stats:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d27770",
   "metadata": {},
   "source": [
    "### Save Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46af435f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saving vector store to ../data/code_vector_store...\n",
      "‚úÖ Vector store saved successfully\n",
      "‚úÖ Vector store saved to ../data/code_vector_store\n"
     ]
    }
   ],
   "source": [
    "# Save vector store for later use\n",
    "vector_store.save(VECTOR_STORE_DIR)\n",
    "print(f\"Vector store saved to {VECTOR_STORE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adc1f6b",
   "metadata": {},
   "source": [
    "## Step 4: Semantic Code Search\n",
    "\n",
    "Search for code using natural language queries.\n",
    "\n",
    "**Example queries:**\n",
    "- \"How to configure the LLM provider?\"\n",
    "- \"Code for processing PDF files\"\n",
    "- \"Functions that handle embeddings\"\n",
    "- \"RAPTOR clustering implementation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450a1b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query: How to configure embeddings and LLM provider?\n",
      "\n",
      "\n",
      "üìã Top 5 Results:\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üèÜ Result 1 (Similarity Score: 1.0741):\n",
      "--------------------------------------------------------------------------------\n",
      "# File: config.py\n",
      "# Class: Config\n",
      "\n",
      "Configuration class for managing LLM providers and API keys.\n",
      "\n",
      "Methods: __init__, _validate_api_keys, get_embedding_model, get_llm_model, __repr__\n",
      "\n",
      "```python\n",
      "class Config:\n",
      "    \"\"\"Configuration class for managing LLM providers and API keys.\"\"\"\n",
      "    \n",
      "    def __init__(self, llm_provider: Optional[str] = None, use_local_embeddings: bool = False):\n",
      "        \"\"\"\n",
      "        Initialize configuration.\n",
      "        \n",
      "        Args:\n",
      "            llm_provider: Either \"openai\" or \"gemini\". If None, reads from env.\n",
      "            use_local_embeddings: If True, use local sentence-transformers instead of API embeddings\n",
      "        \"\"\"\n",
      "        self.llm_provider = llm_provider or os.getenv(\"LLM_PROVIDER\", \"openai\")\n",
      "        self.llm_provider = self.llm_provider.lower()\n",
      "        self.use_local_embeddings = use_local_embeddings\n",
      "        \n",
      "        if self.llm_provider not in [\"openai\", \"gemini\"]:\n",
      "            raise ValueError(f\"Invalid LLM provider: {self.llm_provider}. Must be 'openai' or 'gemini'\")\n",
      "        \n",
      "        # API Keys (only validate if not using local embeddings for everything)\n",
      "        self.openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
      "...\n",
      "```\n",
      "================================================================================\n",
      "\n",
      "üèÜ Result 2 (Similarity Score: 1.1218):\n",
      "--------------------------------------------------------------------------------\n",
      "# File: config.py\n",
      "\n",
      "Configuration module for managing API keys and LLM provider selection.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üèÜ Result 3 (Similarity Score: 1.2892):\n",
      "--------------------------------------------------------------------------------\n",
      "This document describes the structure and functionality of a Python project designed for processing and summarizing PDF documents using Large Language Models (LLMs) and vector stores. The project consists of five modules: `config.py`, `pdf_processor.py`, `raptor.py`, and `vector_store.py`, along with an `__init__.py` file to initialize the `src` package.\n",
      "\n",
      "*   **`config.py`**: This module defines a `Config` class responsible for managing LLM provider selection (either \"openai\" or \"gemini\") and API keys. The `Config` class initializes with an optional `llm_provider` argument (defaults to \"openai\" from environment variable `LLM_PROVIDER`) and a `use_local_embeddings` boolean. It validates the `llm_provider` and retrieves API keys from environment variables. It also includes a `GeminiWrapper` class that inherits from `ChatGoogleGenerativeAI` and overrides the `_generate` method to filter out `max_retries` from kwargs. The `Config` class also contains methods `get_embedding_model` and `get_llm_model` (not fully shown in the provided content) and a `__repr__` method.\n",
      "\n",
      "*   **`pdf_processor.py`**: This module defines a `PDFProcessor` class for extracting text, tables, and images from PDF files. The class initializes with a `Config` instance and uses it to obtain an LLM model. The `PDFProcessor` class has methods for extracting elements (`extract_elements`), summarizing elements (`_summarize_element`), summarizing images (`_summarize_image`), encoding images (`_encode_image`), checking for data absence (`_has_no_data`), creating a page index map (`_get_page_index_map`), processing text and tables in batches (`_process_text_tables_batched`), processing images in batches (`_process_images_batched`), and processing the entire PDF (`process_pdf`). It processes elements in batches of 10 with a 1-second delay between batches.\n",
      "\n",
      "*   **`raptor.py`**: This module implements the RAPTOR (Recursive Abstractive Processing for Tree-Organized Retrieval) algorithm. The `RAPTORProcessor` class performs hierarchical clustering and summarization of text documents. It initializes with a `Config` instance and uses it to obtain embedding and LLM models. The class includes methods for embedding texts (`embed_texts`), performing global and local clustering (`global_cluster_embeddings`, `local_cluster_embeddings`), determining optimal cluster numbers (`get_optimal_clusters`), using Gaussian Mixture Models for clustering (`gmm_cluster`), performing the clustering process (`perform_clustering`), embedding cluster texts (`embed_cluster_texts`), formatting cluster texts (`format_cluster_texts`), embedding and summarizing cluster texts (`embed_cluster_summarize_texts`, `recursive_embed_cluster_summarize`), and processing the entire document (`process`).\n",
      "\n",
      "*   **`vector_store.py`**: This module manages a FAISS vector store for efficient similarity search of document embeddings. The `FAISSVectorStore` class initializes with a `Config` instance and uses it to obtain an embedding model. It provides methods for creating the vector store from texts or documents (`create_from_texts`, `create_from_documents`), adding texts or documents (`add_texts`, `add_documents`), performing similarity searches (`similarity_search`, `similarity_search_with_score`), saving and loading the vector store (`save`, `load`), and getting statistics about the vector store (`get_stats`).\n",
      "\n",
      "*   **`__init__.py`**: This file initializes the `src` package.\n",
      "================================================================================\n",
      "\n",
      "üèÜ Result 4 (Similarity Score: 1.3842):\n",
      "--------------------------------------------------------------------------------\n",
      "# File: vector_store.py\n",
      "# Class: FAISSVectorStore\n",
      "\n",
      "Manage FAISS vector store for document embeddings.\n",
      "\n",
      "Methods: __init__, create_from_texts, create_from_documents, add_texts, add_documents, similarity_search, similarity_search_with_score, save, load, get_stats\n",
      "\n",
      "```python\n",
      "class FAISSVectorStore:\n",
      "    \"\"\"Manage FAISS vector store for document embeddings.\"\"\"\n",
      "    \n",
      "    def __init__(self, config):\n",
      "        \"\"\"\n",
      "        Initialize FAISS vector store.\n",
      "        \n",
      "        Args:\n",
      "            config: Config instance with embedding settings\n",
      "        \"\"\"\n",
      "        self.config = config\n",
      "        self.embeddings = config.get_embedding_model()\n",
      "        self.vector_store = None\n",
      "        \n",
      "    def create_from_texts(\n",
      "        self,\n",
      "        texts: List[str],\n",
      "        metadatas: Optional[List[dict]] = None\n",
      "    ) -> FAISS:\n",
      "        \"\"\"\n",
      "...\n",
      "```\n",
      "================================================================================\n",
      "\n",
      "üèÜ Result 5 (Similarity Score: 1.7205):\n",
      "--------------------------------------------------------------------------------\n",
      "# File: raptor.py\n",
      "# Class: RAPTORProcessor\n",
      "\n",
      "RAPTOR hierarchical text clustering and summarization.\n",
      "\n",
      "Methods: __init__, embed_texts, global_cluster_embeddings, local_cluster_embeddings, get_optimal_clusters, gmm_cluster, perform_clustering, embed_cluster_texts, format_cluster_texts, embed_cluster_summarize_texts, recursive_embed_cluster_summarize, process\n",
      "\n",
      "```python\n",
      "class RAPTORProcessor:\n",
      "    \"\"\"RAPTOR hierarchical text clustering and summarization.\"\"\"\n",
      "    \n",
      "    def __init__(self, config):\n",
      "        \"\"\"\n",
      "        Initialize RAPTOR processor.\n",
      "        \n",
      "        Args:\n",
      "            config: Config instance with LLM and embedding settings\n",
      "        \"\"\"\n",
      "        self.config = config\n",
      "        self.embeddings = config.get_embedding_model()\n",
      "        self.llm = config.get_llm_model(temperature=0)\n",
      "        self.random_seed = config.random_seed\n",
      "        \n",
      "    def embed_texts(self, texts: List[str]) -> np.ndarray:\n",
      "        \"\"\"\n",
      "        Generate embeddings for a list of texts.\n",
      "        \n",
      "        Args:\n",
      "...\n",
      "```\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query = \"How to configure embeddings and LLM provider?\"  # Modify as needed\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "# Search for similar code\n",
    "results = vector_store.similarity_search_with_score(query, k=5)\n",
    "\n",
    "print(f\"\\nTop {len(results)} Results:\\n\")\n",
    "print(\"=\" * 80)\n",
    "for i, (doc, score) in enumerate(results, 1):\n",
    "    print(f\"\\nResult {i} (Similarity Score: {score:.4f}):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(doc.page_content)\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36953a57",
   "metadata": {},
   "source": [
    "### Additional Example Queries\n",
    "\n",
    "Try different types of queries to see the power of RAPTOR retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fdf015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîç Query: PDF processing and extraction\n",
      "================================================================================\n",
      "\n",
      "Result 1 (Score: 0.7581):\n",
      "# File: pdf_processor.py  PDF processing module for extracting text, tables, and images. ...\n",
      "\n",
      "Result 2 (Score: 1.0219):\n",
      "# File: pdf_processor.py # Class: PDFProcessor  Process PDF files to extract text, tables, and images.  Methods: __init__, extract_elements, _summarize_element, _summarize_image, _encode_image, _has_n...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üîç Query: RAPTOR clustering algorithm\n",
      "================================================================================\n",
      "\n",
      "Result 1 (Score: 0.8306):\n",
      "# File: raptor.py # Class: RAPTORProcessor  RAPTOR hierarchical text clustering and summarization.  Methods: __init__, embed_texts, global_cluster_embeddings, local_cluster_embeddings, get_optimal_clu...\n",
      "\n",
      "Result 2 (Score: 0.9739):\n",
      "# File: raptor.py  RAPTOR (Recursive Abstractive Processing for Tree-Organized Retrieval) implementation. Performs hierarchical clustering and summarization of text documents. ...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üîç Query: Vector store implementation\n",
      "================================================================================\n",
      "\n",
      "Result 1 (Score: 1.0766):\n",
      "# File: vector_store.py  FAISS vector store module for efficient similarity search. ...\n",
      "\n",
      "Result 2 (Score: 1.2888):\n",
      "# File: vector_store.py # Class: FAISSVectorStore  Manage FAISS vector store for document embeddings.  Methods: __init__, create_from_texts, create_from_documents, add_texts, add_documents, similarity...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üîç Query: How to handle batch processing?\n",
      "================================================================================\n",
      "\n",
      "Result 1 (Score: 1.2178):\n",
      "# File: pdf_processor.py # Class: PDFProcessor  Process PDF files to extract text, tables, and images.  Methods: __init__, extract_elements, _summarize_element, _summarize_image, _encode_image, _has_n...\n",
      "\n",
      "Result 2 (Score: 1.5549):\n",
      "# File: raptor.py # Class: RAPTORProcessor  RAPTOR hierarchical text clustering and summarization.  Methods: __init__, embed_texts, global_cluster_embeddings, local_cluster_embeddings, get_optimal_clu...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try multiple queries\n",
    "example_queries = [\n",
    "    \"PDF processing and extraction\",\n",
    "    \"RAPTOR clustering algorithm\",\n",
    "    \"Vector store implementation\",\n",
    "    \"How to handle batch processing?\",\n",
    "]\n",
    "\n",
    "for query in example_queries:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    results = vector_store.similarity_search_with_score(query, k=2)\n",
    "    \n",
    "    for i, (doc, score) in enumerate(results, 1):\n",
    "        print(f\"Result {i} (Score: {score:.4f}):\")\n",
    "        # Show first 200 chars of the result\n",
    "        preview = doc.page_content[:200].replace('\\n', ' ')\n",
    "        print(f\"{preview}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74745725",
   "metadata": {},
   "source": [
    "## Advanced: Tree-Level Retrieval\n",
    "\n",
    "RAPTOR enables retrieval at different abstraction levels:\n",
    "- Higher-level results provide summaries\n",
    "- Lower-level results provide specific code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e2e38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query: How does the configuration system work?\n",
      "\n",
      "\n",
      "üìÑ Result 1 (Similarity: -0.360):\n",
      "--------------------------------------------------------------------------------\n",
      "Level: üå≤ Mid-level Summary\n",
      "\n",
      "Content:\n",
      "This document describes the structure and functionality of a Python project designed for processing and summarizing PDF documents using Large Language Models (LLMs) and vector stores. The project consists of five modules: `config.py`, `pdf_processor.py`, `raptor.py`, and `vector_store.py`, along with an `__init__.py` file to initialize the `src` package.\n",
      "\n",
      "*   **`config.py`**: This module defines a...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìÑ Result 2 (Similarity: -0.396):\n",
      "--------------------------------------------------------------------------------\n",
      "Level: üåø Leaf (Original Code)\n",
      "\n",
      "Content:\n",
      "# File: config.py\n",
      "\n",
      "Configuration module for managing API keys and LLM provider selection.\n",
      "...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìÑ Result 3 (Similarity: -0.513):\n",
      "--------------------------------------------------------------------------------\n",
      "Level: üåø Leaf (Original Code)\n",
      "\n",
      "Content:\n",
      "# File: config.py\n",
      "# Class: Config\n",
      "\n",
      "Configuration class for managing LLM providers and API keys.\n",
      "\n",
      "Methods: __init__, _validate_api_keys, get_embedding_model, get_llm_model, __repr__\n",
      "\n",
      "```python\n",
      "class Config:\n",
      "    \"\"\"Configuration class for managing LLM providers and API keys.\"\"\"\n",
      "    \n",
      "    def __init__(self, llm_provider: Optional[str] = None, use_local_embeddings: bool = False):\n",
      "        \"\"\"\n",
      "        In...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def search_with_context(query: str, k: int = 3):\n",
    "    \"\"\"\n",
    "    Search and display results with abstraction level metadata.\n",
    "    \"\"\"\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    \n",
    "    results = vector_store.similarity_search_with_score(query, k=k)\n",
    "    \n",
    "    for i, (doc, score) in enumerate(results, 1):\n",
    "        print(f\"\\nResult {i} (Similarity: {1-score:.3f}):\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Determine if it's original code or a summary\n",
    "        content = doc.page_content\n",
    "        if \"# File:\" in content or \"```python\" in content:\n",
    "            level = \"Leaf (Original Code)\"\n",
    "        elif len(content) < 300:\n",
    "            level = \"High-level Summary\"\n",
    "        else:\n",
    "            level = \"Mid-level Summary\"\n",
    "        \n",
    "        print(f\"Level: {level}\")\n",
    "        print(f\"\\nContent:\\n{content[:400]}...\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Try it out\n",
    "search_with_context(\"How does the configuration system work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e5f19b",
   "metadata": {},
   "source": [
    "## Interactive Code Search\n",
    "\n",
    "Run this cell multiple times with different queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c78974a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query: Show me error handling code\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìÑ Result 1:\n",
      "--------------------------------------------------------------------------------\n",
      "# File: raptor.py\n",
      "# Class: RAPTORProcessor\n",
      "\n",
      "RAPTOR hierarchical text clustering and summarization.\n",
      "\n",
      "Methods: __init__, embed_texts, global_cluster_embeddings, local_cluster_embeddings, get_optimal_clusters, gmm_cluster, perform_clustering, embed_cluster_texts, format_cluster_texts, embed_cluster_summarize_texts, recursive_embed_cluster_summarize, process\n",
      "\n",
      "```python\n",
      "class RAPTORProcessor:\n",
      "    \"\"\"RAPTOR hierarchical text clustering and summarization.\"\"\"\n",
      "    \n",
      "    def __init__(self, config):\n",
      "        \"\"\"\n",
      "        Initialize RAPTOR processor.\n",
      "        \n",
      "        Args:\n",
      "            config: Config instance with LLM and embedding settings\n",
      "        \"\"\"\n",
      "        self.config = config\n",
      "        self.embeddings = config.get_embedding_model()\n",
      "        self.llm = config.get_llm_model(temperature=0)\n",
      "        self.random_seed = config.random_seed\n",
      "        \n",
      "    def embed_texts(self, texts: List[str]) -> np.ndarray:\n",
      "        \"\"\"\n",
      "        Generate embeddings for a list of texts.\n",
      "        \n",
      "        Args:\n",
      "...\n",
      "```\n",
      "================================================================================\n",
      "\n",
      "üìÑ Result 2:\n",
      "--------------------------------------------------------------------------------\n",
      "# File: config.py\n",
      "# Class: GeminiWrapper\n",
      "\n",
      "No description\n",
      "\n",
      "Methods: _generate\n",
      "\n",
      "```python\n",
      "            class GeminiWrapper(ChatGoogleGenerativeAI):\n",
      "                def _generate(self, messages, stop=None, run_manager=None, **kwargs):\n",
      "                    # Remove max_retries from kwargs before passing to parent\n",
      "                    kwargs_filtered = {k: v for k, v in kwargs.items() if k != 'max_retries'}\n",
      "                    return super()._generate(messages, stop=stop, run_manager=run_manager, **kwargs_filtered)\n",
      "            \n",
      "            return GeminiWrapper(\n",
      "                model=self.model_name,\n",
      "                temperature=temperature,\n",
      "                google_api_key=self.google_api_key,\n",
      "                max_output_tokens=max_tokens\n",
      "            )\n",
      "    \n",
      "    def __repr__(self):\n",
      "        return f\"Config(provider={self.llm_provider}, model={self.model_name})\"\n",
      "\n",
      "...\n",
      "```\n",
      "================================================================================\n",
      "\n",
      "üìÑ Result 3:\n",
      "--------------------------------------------------------------------------------\n",
      "# File: config.py\n",
      "# Class: Config\n",
      "\n",
      "Configuration class for managing LLM providers and API keys.\n",
      "\n",
      "Methods: __init__, _validate_api_keys, get_embedding_model, get_llm_model, __repr__\n",
      "\n",
      "```python\n",
      "class Config:\n",
      "    \"\"\"Configuration class for managing LLM providers and API keys.\"\"\"\n",
      "    \n",
      "    def __init__(self, llm_provider: Optional[str] = None, use_local_embeddings: bool = False):\n",
      "        \"\"\"\n",
      "        Initialize configuration.\n",
      "        \n",
      "        Args:\n",
      "            llm_provider: Either \"openai\" or \"gemini\". If None, reads from env.\n",
      "            use_local_embeddings: If True, use local sentence-transformers instead of API embeddings\n",
      "        \"\"\"\n",
      "        self.llm_provider = llm_provider or os.getenv(\"LLM_PROVIDER\", \"openai\")\n",
      "        self.llm_provider = self.llm_provider.lower()\n",
      "        self.use_local_embeddings = use_local_embeddings\n",
      "        \n",
      "        if self.llm_provider not in [\"openai\", \"gemini\"]:\n",
      "            raise ValueError(f\"Invalid LLM provider: {self.llm_provider}. Must be 'openai' or 'gemini'\")\n",
      "        \n",
      "        # API Keys (only validate if not using local embeddings for everything)\n",
      "        self.openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
      "...\n",
      "```\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Interactive search\n",
    "your_query = \"Show me error handling code\"  # Modify as needed\n",
    "\n",
    "results = vector_store.similarity_search(your_query, k=3)\n",
    "\n",
    "print(f\"Query: {your_query}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(doc.page_content)\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e906bc3",
   "metadata": {},
   "source": [
    "## Load Existing Vector Store (Optional)\n",
    "\n",
    "Load previously saved vector store to skip processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e39422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to load existing vector store\n",
    "# vector_store_loaded = FAISSVectorStore(config)\n",
    "# vector_store_loaded.load(VECTOR_STORE_DIR)\n",
    "\n",
    "# print(\"Vector store loaded successfully\")\n",
    "# print(f\"Stats: {vector_store_loaded.get_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590d0085",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Successfully completed:\n",
    "- Extracted code from Python codebase\n",
    "- Applied RAPTOR hierarchical clustering\n",
    "- Created FAISS vector store for code search\n",
    "- Performed semantic code retrieval\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Try different codebases** - Point to any Python project\n",
    "2. **Integrate with RAG** - Build code Q&A systems\n",
    "3. **Code documentation** - Generate docs from RAPTOR summaries\n",
    "4. **Code search tools** - Build IDE plugins\n",
    "5. **Multi-language support** - Extend to JavaScript, Java, etc.\n",
    "\n",
    "### Optimization Tips:\n",
    "\n",
    "- **Larger codebases**: Use `max_files` parameter or filters\n",
    "- **Better results**: Include imports and comments in extraction\n",
    "- **Custom embeddings**: Try code-specific embedding models\n",
    "- **Fine-tune retrieval**: Adjust `k` parameter and similarity thresholds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
