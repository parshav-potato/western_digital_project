{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4b98b90",
   "metadata": {},
   "source": [
    "# RAPTOR Code Retrieval Demo\n",
    "\n",
    "This notebook demonstrates hierarchical code retrieval using RAPTOR clustering:\n",
    "1. Extract code from Python files\n",
    "2. Apply RAPTOR clustering for multi-level code understanding\n",
    "3. Store embeddings in FAISS vector store\n",
    "4. Retrieve relevant code snippets via semantic search\n",
    "\n",
    "**Key Features**:\n",
    "- Tree-structured retrieval at different abstraction levels\n",
    "- Semantic code search using natural language queries\n",
    "- Hierarchical summaries for codebase understanding\n",
    "- Local embeddings (no API costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d905c8",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "**Prerequisites:**\n",
    "1. Run `uv sync` in project root\n",
    "2. Copy `.env.example` to `.env` and configure API keys\n",
    "3. Specify Python codebase path for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24091df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parshav-potato/projects/western_digital_project/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path to import src modules\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# Remove cached modules to force fresh import\n",
    "modules_to_remove = [key for key in sys.modules.keys() if key.startswith('src.')]\n",
    "for module in modules_to_remove:\n",
    "    del sys.modules[module]\n",
    "\n",
    "# Import our modules\n",
    "from src.config import Config\n",
    "from src.raptor import RAPTORProcessor\n",
    "from src.vector_store import FAISSVectorStore\n",
    "from src.code_processor import CodeProcessor\n",
    "\n",
    "print(\"Modules loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538880b3",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Choose LLM provider for code summaries: `\"openai\"` or `\"gemini\"`\n",
    "\n",
    "Embeddings use local sentence-transformers (no API calls required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98a59a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded: Config(provider=gemini, model=gemini-2.0-flash)\n",
      "Embeddings: Local (sentence-transformers)\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "LLM_PROVIDER = \"gemini\"  # or \"openai\"\n",
    "USE_LOCAL_EMBEDDINGS = True  # Use free local embeddings\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config(llm_provider=LLM_PROVIDER, use_local_embeddings=USE_LOCAL_EMBEDDINGS)\n",
    "print(f\"Configuration loaded: {config}\")\n",
    "print(f\"Embeddings: {'Local (sentence-transformers)' if USE_LOCAL_EMBEDDINGS else f'{LLM_PROVIDER} API'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4892b0",
   "metadata": {},
   "source": [
    "## Set Codebase Path\n",
    "\n",
    "Specify the Python codebase directory for analysis (default: `../src`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c772665c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codebase path: ../src\n",
      "Vector store directory: ../data/code_vector_store\n"
     ]
    }
   ],
   "source": [
    "# Set your codebase path\n",
    "CODEBASE_PATH = \"../src\"  # Modify as needed\n",
    "\n",
    "# Output directories\n",
    "VECTOR_STORE_DIR = \"../data/code_vector_store\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(VECTOR_STORE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Codebase path: {CODEBASE_PATH}\")\n",
    "print(f\"Vector store directory: {VECTOR_STORE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3609d2a",
   "metadata": {},
   "source": [
    "## Step 1: Extract Code from Codebase\n",
    "\n",
    "Process:\n",
    "1. Find all Python files\n",
    "2. Extract functions and classes with docstrings\n",
    "3. Create structured code chunks for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62122b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 Python files\n",
      "\n",
      "Extracted 12 code chunks\n",
      "\n",
      "Sample code chunk:\n",
      "\n",
      "# File: config.py\n",
      "\n",
      "Configuration module for managing API keys and LLM provider selection.\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Initialize code processor\n",
    "code_processor = CodeProcessor()\n",
    "\n",
    "# Extract code chunks\n",
    "code_chunks = code_processor.extract_code_chunks(CODEBASE_PATH)\n",
    "\n",
    "print(f\"\\nExtracted {len(code_chunks)} code chunks\")\n",
    "print(f\"\\nSample code chunk:\\n\")\n",
    "print(code_chunks[0][:500] + \"...\" if code_chunks else \"No chunks found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fd9cab",
   "metadata": {},
   "source": [
    "## Step 2: Apply RAPTOR Clustering\n",
    "\n",
    "RAPTOR creates hierarchical tree structure:\n",
    "- **Level 0 (Leaf)**: Individual functions/classes\n",
    "- **Level 1**: Summaries of related code groups\n",
    "- **Level 2**: High-level module summaries\n",
    "- **Level 3**: Overall codebase understanding\n",
    "\n",
    "Enables retrieval at different abstraction levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f410553e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Using local embeddings (sentence-transformers/all-MiniLM-L6-v2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parshav-potato/projects/western_digital_project/demo_code/src/config.py:58: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  return HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building RAPTOR tree structure...\n",
      "Creating hierarchical summaries of code.\n",
      "\n",
      "\n",
      "Building RAPTOR tree with 3 levels...\n",
      "Starting with 12 leaf texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parshav-potato/projects/western_digital_project/.venv/lib/python3.12/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Level 1: Generated 2 clusters\n",
      "  Level 2: Generated 1 clusters\n",
      "  Level 1: Added 2 summaries\n",
      "  Level 2: Added 1 summaries\n",
      "RAPTOR processing complete: 15 total texts\n",
      "\n",
      "RAPTOR Results:\n",
      "  Original code chunks: 12\n",
      "  Total texts (with summaries): 15\n",
      "  New summaries created: 3\n"
     ]
    }
   ],
   "source": [
    "# Initialize RAPTOR processor\n",
    "raptor = RAPTORProcessor(config)\n",
    "\n",
    "# Apply RAPTOR clustering (3 levels of hierarchy)\n",
    "print(\"Building RAPTOR tree structure...\")\n",
    "print(\"Creating hierarchical summaries of code.\\n\")\n",
    "\n",
    "all_code_texts = raptor.process(texts=code_chunks, n_levels=3)\n",
    "\n",
    "print(f\"\\nRAPTOR Results:\")\n",
    "print(f\"  Original code chunks: {len(code_chunks)}\")\n",
    "print(f\"  Total texts (with summaries): {len(all_code_texts)}\")\n",
    "print(f\"  New summaries created: {len(all_code_texts) - len(code_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97a5de8",
   "metadata": {},
   "source": [
    "## Step 3: Create FAISS Vector Store\n",
    "\n",
    "Store code chunks and summaries in FAISS vector database for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ae7d1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Using local embeddings (sentence-transformers/all-MiniLM-L6-v2)\n",
      "Creating vector store from code embeddings...\n",
      "\n",
      "Creating FAISS vector store from 15 texts...\n",
      "Vector store created successfully\n",
      "\n",
      "Vector Store Stats:\n",
      "  status: initialized\n",
      "  n_vectors: 15\n",
      "  embedding_provider: gemini\n"
     ]
    }
   ],
   "source": [
    "# Initialize vector store\n",
    "vector_store = FAISSVectorStore(config)\n",
    "\n",
    "# Create vector store from all texts\n",
    "print(\"Creating vector store from code embeddings...\")\n",
    "vector_store.create_from_texts(all_code_texts)\n",
    "\n",
    "# Display stats\n",
    "stats = vector_store.get_stats()\n",
    "print(f\"\\nVector Store Stats:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d27770",
   "metadata": {},
   "source": [
    "### Save Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46af435f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving vector store to ../data/code_vector_store...\n",
      "Vector store saved successfully\n",
      "Vector store saved to ../data/code_vector_store\n"
     ]
    }
   ],
   "source": [
    "# Save vector store for later use\n",
    "vector_store.save(VECTOR_STORE_DIR)\n",
    "print(f\"Vector store saved to {VECTOR_STORE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adc1f6b",
   "metadata": {},
   "source": [
    "## Step 4: Semantic Code Search\n",
    "\n",
    "Search for code using natural language queries.\n",
    "\n",
    "**Example queries:**\n",
    "- \"How to configure the LLM provider?\"\n",
    "- \"Code for processing PDF files\"\n",
    "- \"Functions that handle embeddings\"\n",
    "- \"RAPTOR clustering implementation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "450a1b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How to configure embeddings and LLM provider?\n",
      "\n",
      "\n",
      "Top 5 Results:\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Result 1 (Similarity Score: 1.0741):\n",
      "--------------------------------------------------------------------------------\n",
      "# File: config.py\n",
      "# Class: Config\n",
      "\n",
      "Configuration class for managing LLM providers and API keys.\n",
      "\n",
      "Methods: __init__, _validate_api_keys, get_embedding_model, get_llm_model, __repr__\n",
      "\n",
      "```python\n",
      "class Config:\n",
      "    \"\"\"Configuration class for managing LLM providers and API keys.\"\"\"\n",
      "    \n",
      "    def __init__(self, llm_provider: Optional[str] = None, use_local_embeddings: bool = False):\n",
      "        \"\"\"\n",
      "        Initialize configuration.\n",
      "        \n",
      "        Args:\n",
      "            llm_provider: Either \"openai\" or \"gemini\". If None, reads from env.\n",
      "            use_local_embeddings: If True, use local sentence-transformers instead of API embeddings\n",
      "        \"\"\"\n",
      "        self.llm_provider = llm_provider or os.getenv(\"LLM_PROVIDER\", \"openai\")\n",
      "        self.llm_provider = self.llm_provider.lower()\n",
      "        self.use_local_embeddings = use_local_embeddings\n",
      "        \n",
      "        if self.llm_provider not in [\"openai\", \"gemini\"]:\n",
      "            raise ValueError(f\"Invalid LLM provider: {self.llm_provider}. Must be 'openai' or 'gemini'\")\n",
      "        \n",
      "        # API Keys (only validate if not using local embeddings for everything)\n",
      "        self.openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
      "...\n",
      "```\n",
      "================================================================================\n",
      "\n",
      "Result 2 (Similarity Score: 1.1084):\n",
      "--------------------------------------------------------------------------------\n",
      "The provided content describes three Python modules: `config.py`, `raptor.py`, and `vector_store.py`.\n",
      "\n",
      "**config.py:** This module defines a `Config` class for managing LLM providers (either \"openai\" or \"gemini\") and API keys. The `Config` class initializes with an optional `llm_provider` argument (defaults to \"openai\" or reads from the environment variable `LLM_PROVIDER`) and a `use_local_embeddings` boolean. It validates the `llm_provider` and retrieves API keys from environment variables. It also includes a `GeminiWrapper` class, which inherits from `ChatGoogleGenerativeAI` and overrides the `_generate` method to filter out `max_retries` from kwargs before passing them to the parent class. The `Config` class also has methods `get_embedding_model` and `get_llm_model` (not fully shown in the provided content) and a `__repr__` method.\n",
      "\n",
      "**raptor.py:** This module implements the RAPTOR (Recursive Abstractive Processing for Tree-Organized Retrieval) algorithm. It defines a `RAPTORProcessor` class for hierarchical text clustering and summarization. The `RAPTORProcessor` class initializes with a `Config` instance, embedding model, LLM (with temperature 0), and a random seed. It includes methods for embedding texts (`embed_texts`), and other clustering and summarization steps (not fully shown in the provided content).\n",
      "\n",
      "**vector_store.py:** This module manages a FAISS vector store for efficient similarity search. It defines a `FAISSVectorStore` class, which initializes with a `Config` instance and an embedding model. It provides methods for creating the vector store from texts or documents (`create_from_texts`, `create_from_documents`), adding texts or documents (`add_texts`, `add_documents`), performing similarity searches (`similarity_search`, `similarity_search_with_score`), saving and loading the vector store (`save`, `load`), and getting statistics (`get_stats`).\n",
      "================================================================================\n",
      "\n",
      "Result 3 (Similarity Score: 1.1218):\n",
      "--------------------------------------------------------------------------------\n",
      "# File: config.py\n",
      "\n",
      "Configuration module for managing API keys and LLM provider selection.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Result 4 (Similarity Score: 1.3842):\n",
      "--------------------------------------------------------------------------------\n",
      "# File: vector_store.py\n",
      "# Class: FAISSVectorStore\n",
      "\n",
      "Manage FAISS vector store for document embeddings.\n",
      "\n",
      "Methods: __init__, create_from_texts, create_from_documents, add_texts, add_documents, similarity_search, similarity_search_with_score, save, load, get_stats\n",
      "\n",
      "```python\n",
      "class FAISSVectorStore:\n",
      "    \"\"\"Manage FAISS vector store for document embeddings.\"\"\"\n",
      "    \n",
      "    def __init__(self, config):\n",
      "        \"\"\"\n",
      "        Initialize FAISS vector store.\n",
      "        \n",
      "        Args:\n",
      "            config: Config instance with embedding settings\n",
      "        \"\"\"\n",
      "        self.config = config\n",
      "        self.embeddings = config.get_embedding_model()\n",
      "        self.vector_store = None\n",
      "        \n",
      "    def create_from_texts(\n",
      "        self,\n",
      "        texts: List[str],\n",
      "        metadatas: Optional[List[dict]] = None\n",
      "    ) -> FAISS:\n",
      "        \"\"\"\n",
      "...\n",
      "```\n",
      "================================================================================\n",
      "\n",
      "Result 5 (Similarity Score: 1.3890):\n",
      "--------------------------------------------------------------------------------\n",
      "The provided content describes five Python modules: `config.py`, `raptor.py`, `vector_store.py`, `pdf_processor.py`, and `code_processor.py`, along with an `__init__.py` file.\n",
      "\n",
      "*   **`config.py`**: Defines a `Config` class for managing LLM providers (\"openai\" or \"gemini\") and API keys, retrieved from environment variables. It validates the provider and includes a `GeminiWrapper` class that inherits from `ChatGoogleGenerativeAI` and overrides the `_generate` method to filter out `max_retries`. It also includes methods `get_embedding_model` and `get_llm_model` and a `__repr__` method.\n",
      "\n",
      "*   **`raptor.py`**: Implements the RAPTOR algorithm via the `RAPTORProcessor` class for hierarchical text clustering and summarization. The class initializes with a `Config` instance, embedding model, LLM (temperature 0), and a random seed. It includes an `embed_texts` method.\n",
      "\n",
      "*   **`vector_store.py`**: Manages a FAISS vector store using the `FAISSVectorStore` class for efficient similarity search. It initializes with a `Config` instance and an embedding model. It provides methods for creating the vector store (`create_from_texts`, `create_from_documents`), adding data (`add_texts`, `add_documents`), searching (`similarity_search`, `similarity_search_with_score`), saving/loading (`save`, `load`), and getting statistics (`get_stats`).\n",
      "\n",
      "*   **`pdf_processor.py`**: Contains the `PDFProcessor` class, designed to extract text, tables, and images from PDF files. The class has methods for initialization (`__init__`) which uses a `config` object to initialize an LLM model with a temperature of 0 and a maximum of 1024 tokens, extracting elements (`extract_elements`), and several private helper methods. It also sets a `batch_size` of 10 and a `delay_between_batches` of 1 second.\n",
      "\n",
      "*   **`code_processor.py`**: Contains the `CodeProcessor` class, which extracts and processes code from Python codebases. The class has methods for initialization (`__init__`), extracting code chunks (`extract_code_chunks`), and several private helper methods, as well as a method for getting file statistics (`get_file_stats`). The `extract_code_chunks` method extracts module-level docstrings, classes (with docstrings, methods, and source code), and top-level functions (with docstrings, arguments, and source code).\n",
      "\n",
      "*   **`__init__.py`**: Initializes the `src` package.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query = \"How to configure embeddings and LLM provider?\"  # Modify as needed\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "# Search for similar code\n",
    "results = vector_store.similarity_search_with_score(query, k=5)\n",
    "\n",
    "print(f\"\\nTop {len(results)} Results:\\n\")\n",
    "print(\"=\" * 80)\n",
    "for i, (doc, score) in enumerate(results, 1):\n",
    "    print(f\"\\nResult {i} (Similarity Score: {score:.4f}):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(doc.page_content)\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36953a57",
   "metadata": {},
   "source": [
    "### Additional Example Queries\n",
    "\n",
    "Try different types of queries to see the power of RAPTOR retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5fdf015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Query: PDF processing and extraction\n",
      "================================================================================\n",
      "\n",
      "Result 1 (Score: 0.7581):\n",
      "# File: pdf_processor.py  PDF processing module for extracting text, tables, and images. ...\n",
      "\n",
      "Result 2 (Score: 0.9912):\n",
      "This content describes two Python modules: `pdf_processor.py` and `code_processor.py`, along with an `__init__.py` file.  *   **`pdf_processor.py`**: This module contains the `PDFProcessor` class, des...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Query: RAPTOR clustering algorithm\n",
      "================================================================================\n",
      "\n",
      "Result 1 (Score: 0.8306):\n",
      "# File: raptor.py # Class: RAPTORProcessor  RAPTOR hierarchical text clustering and summarization.  Methods: __init__, embed_texts, global_cluster_embeddings, local_cluster_embeddings, get_optimal_clu...\n",
      "\n",
      "Result 2 (Score: 0.9739):\n",
      "# File: raptor.py  RAPTOR (Recursive Abstractive Processing for Tree-Organized Retrieval) implementation. Performs hierarchical clustering and summarization of text documents. ...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Query: Vector store implementation\n",
      "================================================================================\n",
      "\n",
      "Result 1 (Score: 1.0766):\n",
      "# File: vector_store.py  FAISS vector store module for efficient similarity search. ...\n",
      "\n",
      "Result 2 (Score: 1.2888):\n",
      "# File: vector_store.py # Class: FAISSVectorStore  Manage FAISS vector store for document embeddings.  Methods: __init__, create_from_texts, create_from_documents, add_texts, add_documents, similarity...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Query: How to handle batch processing?\n",
      "================================================================================\n",
      "\n",
      "Result 1 (Score: 1.1564):\n",
      "This content describes two Python modules: `pdf_processor.py` and `code_processor.py`, along with an `__init__.py` file.  *   **`pdf_processor.py`**: This module contains the `PDFProcessor` class, des...\n",
      "\n",
      "Result 2 (Score: 1.2178):\n",
      "# File: pdf_processor.py # Class: PDFProcessor  Process PDF files to extract text, tables, and images.  Methods: __init__, extract_elements, _summarize_element, _summarize_image, _encode_image, _has_n...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try multiple queries\n",
    "example_queries = [\n",
    "    \"PDF processing and extraction\",\n",
    "    \"RAPTOR clustering algorithm\",\n",
    "    \"Vector store implementation\",\n",
    "    \"How to handle batch processing?\",\n",
    "]\n",
    "\n",
    "for query in example_queries:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    results = vector_store.similarity_search_with_score(query, k=2)\n",
    "    \n",
    "    for i, (doc, score) in enumerate(results, 1):\n",
    "        print(f\"Result {i} (Score: {score:.4f}):\")\n",
    "        # Show first 200 chars of the result\n",
    "        preview = doc.page_content[:200].replace('\\n', ' ')\n",
    "        print(f\"{preview}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74745725",
   "metadata": {},
   "source": [
    "## Advanced: Tree-Level Retrieval\n",
    "\n",
    "RAPTOR enables retrieval at different abstraction levels:\n",
    "- Higher-level results provide summaries\n",
    "- Lower-level results provide specific code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77e2e38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How does the configuration system work?\n",
      "\n",
      "\n",
      "Result 1 (Similarity: -0.206):\n",
      "--------------------------------------------------------------------------------\n",
      "Level: Mid-level Summary\n",
      "\n",
      "Content:\n",
      "The provided content describes three Python modules: `config.py`, `raptor.py`, and `vector_store.py`.\n",
      "\n",
      "**config.py:** This module defines a `Config` class for managing LLM providers (either \"openai\" or \"gemini\") and API keys. The `Config` class initializes with an optional `llm_provider` argument (defaults to \"openai\" or reads from the environment variable `LLM_PROVIDER`) and a `use_local_embeddin...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result 2 (Similarity: -0.280):\n",
      "--------------------------------------------------------------------------------\n",
      "Level: Mid-level Summary\n",
      "\n",
      "Content:\n",
      "The provided content describes five Python modules: `config.py`, `raptor.py`, `vector_store.py`, `pdf_processor.py`, and `code_processor.py`, along with an `__init__.py` file.\n",
      "\n",
      "*   **`config.py`**: Defines a `Config` class for managing LLM providers (\"openai\" or \"gemini\") and API keys, retrieved from environment variables. It validates the provider and includes a `GeminiWrapper` class that inherit...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result 3 (Similarity: -0.396):\n",
      "--------------------------------------------------------------------------------\n",
      "Level: Leaf (Original Code)\n",
      "\n",
      "Content:\n",
      "# File: config.py\n",
      "\n",
      "Configuration module for managing API keys and LLM provider selection.\n",
      "...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def search_with_context(query: str, k: int = 3):\n",
    "    \"\"\"\n",
    "    Search and display results with abstraction level metadata.\n",
    "    \"\"\"\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    \n",
    "    results = vector_store.similarity_search_with_score(query, k=k)\n",
    "    \n",
    "    for i, (doc, score) in enumerate(results, 1):\n",
    "        print(f\"\\nResult {i} (Similarity: {1-score:.3f}):\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Determine if it's original code or a summary\n",
    "        content = doc.page_content\n",
    "        if \"# File:\" in content or \"```python\" in content:\n",
    "            level = \"Leaf (Original Code)\"\n",
    "        elif len(content) < 300:\n",
    "            level = \"High-level Summary\"\n",
    "        else:\n",
    "            level = \"Mid-level Summary\"\n",
    "        \n",
    "        print(f\"Level: {level}\")\n",
    "        print(f\"\\nContent:\\n{content[:400]}...\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Try it out\n",
    "search_with_context(\"How does the configuration system work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e5f19b",
   "metadata": {},
   "source": [
    "## Interactive Code Search\n",
    "\n",
    "Run this cell multiple times with different queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c78974a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Show me error handling code\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Result 1:\n",
      "--------------------------------------------------------------------------------\n",
      "# File: code_processor.py\n",
      "\n",
      "Code processing module for extracting code from Python files.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Result 2:\n",
      "--------------------------------------------------------------------------------\n",
      "# File: code_processor.py\n",
      "# Class: CodeProcessor\n",
      "\n",
      "Extract and process code from Python codebases.\n",
      "\n",
      "Methods: __init__, extract_code_chunks, _extract_from_file, _extract_class, _extract_function, get_file_stats\n",
      "\n",
      "```python\n",
      "class CodeProcessor:\n",
      "    \"\"\"Extract and process code from Python codebases.\"\"\"\n",
      "    \n",
      "    def __init__(self):\n",
      "        \"\"\"Initialize code processor.\"\"\"\n",
      "        pass\n",
      "    \n",
      "    def extract_code_chunks(\n",
      "        self, \n",
      "        codebase_path: str, \n",
      "        max_files: Optional[int] = None\n",
      "    ) -> List[str]:\n",
      "        \"\"\"\n",
      "        Extract code chunks from Python files in a codebase.\n",
      "        \n",
      "        Extracts:\n",
      "        - Module-level docstrings\n",
      "        - Classes with their docstrings, methods, and source code\n",
      "        - Top-level functions with docstrings, arguments, and source code\n",
      "        \n",
      "...\n",
      "```\n",
      "================================================================================\n",
      "\n",
      "Result 3:\n",
      "--------------------------------------------------------------------------------\n",
      "# File: raptor.py\n",
      "# Class: RAPTORProcessor\n",
      "\n",
      "RAPTOR hierarchical text clustering and summarization.\n",
      "\n",
      "Methods: __init__, embed_texts, global_cluster_embeddings, local_cluster_embeddings, get_optimal_clusters, gmm_cluster, perform_clustering, embed_cluster_texts, format_cluster_texts, embed_cluster_summarize_texts, recursive_embed_cluster_summarize, process\n",
      "\n",
      "```python\n",
      "class RAPTORProcessor:\n",
      "    \"\"\"RAPTOR hierarchical text clustering and summarization.\"\"\"\n",
      "    \n",
      "    def __init__(self, config):\n",
      "        \"\"\"\n",
      "        Initialize RAPTOR processor.\n",
      "        \n",
      "        Args:\n",
      "            config: Config instance with LLM and embedding settings\n",
      "        \"\"\"\n",
      "        self.config = config\n",
      "        self.embeddings = config.get_embedding_model()\n",
      "        self.llm = config.get_llm_model(temperature=0)\n",
      "        self.random_seed = config.random_seed\n",
      "        \n",
      "    def embed_texts(self, texts: List[str]) -> np.ndarray:\n",
      "        \"\"\"\n",
      "        Generate embeddings for a list of texts.\n",
      "        \n",
      "        Args:\n",
      "...\n",
      "```\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Interactive search\n",
    "your_query = \"Show me error handling code\"  # Modify as needed\n",
    "\n",
    "results = vector_store.similarity_search(your_query, k=3)\n",
    "\n",
    "print(f\"Query: {your_query}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(doc.page_content)\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e906bc3",
   "metadata": {},
   "source": [
    "## Load Existing Vector Store (Optional)\n",
    "\n",
    "Load previously saved vector store to skip processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13e39422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to load existing vector store\n",
    "# vector_store_loaded = FAISSVectorStore(config)\n",
    "# vector_store_loaded.load(VECTOR_STORE_DIR)\n",
    "\n",
    "# print(\"Vector store loaded successfully\")\n",
    "# print(f\"Stats: {vector_store_loaded.get_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590d0085",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Successfully completed:\n",
    "- Extracted code from Python codebase\n",
    "- Applied RAPTOR hierarchical clustering\n",
    "- Created FAISS vector store for code search\n",
    "- Performed semantic code retrieval\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Try different codebases** - Point to any Python project\n",
    "2. **Integrate with RAG** - Build code Q&A systems\n",
    "3. **Code documentation** - Generate docs from RAPTOR summaries\n",
    "4. **Code search tools** - Build IDE plugins\n",
    "5. **Multi-language support** - Extend to JavaScript, Java, etc.\n",
    "\n",
    "### Optimization Tips:\n",
    "\n",
    "- **Larger codebases**: Use `max_files` parameter or filters\n",
    "- **Better results**: Include imports and comments in extraction\n",
    "- **Custom embeddings**: Try code-specific embedding models\n",
    "- **Fine-tune retrieval**: Adjust `k` parameter and similarity thresholds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
